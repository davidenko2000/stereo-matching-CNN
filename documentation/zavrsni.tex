\documentclass[times, utf8, zavrsni, numeric]{fer}
\usepackage{booktabs}
\usepackage{pdfpages}
\usepackage{amsmath}
\usepackage{empheq}
\usepackage{etoolbox}
\begin{document}

% TODO: Navedite broj rada.
\thesisnumber{482}

% TODO: Navedite naslov rada.
\title{Korespondencijska ugrađivanja za stereoskopsku rekonstrukciju}

% TODO: Navedite vaše ime i prezime.
\author{David Kerman}

\maketitle

\includepdf[pages=-]{izvornik.pdf}

% Dodavanje zahvale ili prazne stranice. Ako ne želite dodati zahvalu, naredbu ostavite radi prazne stranice.
\zahvala{Zahvaljujem mentoru prof.~dr.~sc.~Siniši Šegviću, na savjetima i ukazanoj pomoći. Zahvaljujem kolegi Antoniu Ilinoviću na diskusijama i pomoći tijekom izrade rada. Također, zahvaljujem svojoj obitelji, prijateljima i kolegama na potpori tijekom mojeg dosadašnjeg obrazovanja.}

\tableofcontents

\chapter{Uvod}
Računalni vid kao jedna od temeljnih grana umjetne inteligencije omogućava računalima da izluče značajnu informaciju iz slika, videa ili sličnih vizualnih ulaza. Područje stereoskopske rekonstrukcije ima za cilj ostvariti stvaran trodimenzionalan položaj točaka koje su promatrane u dvije ili više slika, te kao takvo ima vrlo široke primjene uključujući one za određivanje sličnosti slikovnih okana. Kroz povijest postojale su klasične metode za određivanje korespodencijskih metrika, no pojavom snažnijih računala iskorišteno je duboko učenje kao alat kojim je moguće korespodencijske metrike naučiti na stvarnim podacima. Za potrebe metoda temeljenih na ugrađivanju u visokodimenzionalni prostor potrebne su rektificirane stereoskopske slike s poznatom dubinskom informacijom.\\

U okviru ovog radu opisana je geometrija stereoskopskog para, kalibracija, rektifikacija i algoritmi korišteni za ostvarivanje korespodencije. Zatim definirani su osnovni pojmovi u dubokom učenju, te postupci korišteni u dubokom učenju uz memorijske zahtjeve. Predstavljen je podatkovni skup na kojem su izvršeni postupci treniranja i validacije. Opisan je model kojim je ostvareno korespodencijsko ugrađivanje slikovnih okana, koji su korišteni u stereoskopskoj rekonstrukciji. Nastavno na to, opisan je postupak učenja takvog modela, te eksperimentalno vrednovanje rekonstrukcijske točnosti.

\chapter{Stereoskopska rekonstrukcija}
\section{Epipolarna geometrija}
Epipolarna geometrija je geometrija koja ima primjenu u stereoskopskoj rekonstrukciji, a po definiciji opisuje odnos točaka u slikama dobivenih iz dviju kamera. Također naziva se i epipolarno ograničenje jer uvelike smanjuje broj točaka koje je potrebno provjeriti pri traženju korespodentnih točaka. Točka $X$ koja je prikazana na slici 2.1 prikazuje točku u 3D prostoru koja se snima iz dviju kamera. Točke $C$ i $C'$ prikazuju centre lijeve, odnosno desne kamere. Projekcija točke $X$ na ravninu lijeve kamere je $x$, a na ravninu desne kamere $x'$. Važno je napomenuti da su točke $X$, $x$ i $x'$ u istoj ravnini, te zajedno sa centrima kamera $C$ i $C'$ čine tzv. \textit{epipolarnu ravninu} $\pi$.\\
\begin{figure}[htb]
\centering
\includegraphics[scale=0.46]{img/slika1.png}
\caption{Epipolarna ravnina u kojoj se nalaze točka $X$,te točke nastale projekcijom $x$ i $x'$}
\label{fig:Epipolar}
\end{figure}\\
Nadalje, na slici su označeni \textit{epipolovi} oznakama $e$ i $e'$ te oni prikazuju točke u kojima pravac koji povezuje centre kamera $C$ i $C'$ siječe slikovne ravnine.
Najvažniji element su \textit{epipolarne linije}, označene dužinama koje spajaju $x$ i $e$, te $x'$ i $e'$, ali su zapravo pravci koji su nastali presjekom epipolarne ravnine sa slikovnom ravninom. Epipol je točka u kojoj se sijeku svi epipolarni pravci.\\
Poznavajući činjenicu da se točke nalaze u istoj ravnini, te ako znamo položaj točke $x$ možemo odrediti položaj točke $x'$. Rješenja koja se moraju ispitati da bi saznali položaj točke $x'$ nalaze se na epipolarnoj liniji, što uvelike olakšava pronalaženje korespodentnih točaka.

\section{Pretprocesiranje slika}
Parametre geometrije sustava kamera moguće je  podijeliti na dvije vrste: intrinzični i ekstrinzični. Ekstrinzični parametri su oni koji opisuju odnos svojstva koja su zajednička kamerama. Intrinzični pak opisuju svojstva kamera koje se odnose svaku kameru pojedinačno. Kao primjere za intrinzične faktore navodimo svojstva koja utječu na nji, a to su nesavršenost leća (na slici 2.2 prikazano je radijalno izobličenje), pomak senzora od centra leća te slične fizičke karakteristike nesavršenosti kamera.\\
\begin{figure}[htb]
\centering
\includegraphics[scale=0.7]{img/slika2.png}
\caption{Primjeri radijalnog izobličenja. Izvor:\citep{bunjevac2017stereo}}
\label{fig:Radial}
\end{figure}\\
S druge strane ekstrinzični parametri nas dovode do nužnih transformacija kojima se slike dovode u istu ravninu projekcije te se time postiže da su pikseli duž horizontalnog pravca jedne slike na istoj visini na drugoj slici. Taj pravac je već spomenut, riječ je o epipolarnoj liniji. Ovakvim transformacijama uvelike se olakšava pronalaženje korespodentnih točaka koje u ovom slučaju treba tražiti samo duž jedne linije, epipolarne linije. Problem je sveden na jednu dimenziju. Kalibracijom se riješava problem nesavršenosti leća, odnosno problem izobličenja slika.\\
\\

\subsection{Rektifikacija}
Za postupak rektifikacije važni su ekstrinzični faktori kamera, dok na njihov izračun utječu intrinzični faktori. Transformacijom slika rektifikacijom dobivamo slike čiji pikseli koji u prostoru odgovaraju točkama na istoj visini,a nalaze se duž iste epipolarne linije. Također, nakon provedenih transformacija intrinzični parametri kamera postaju jednaki.\\
\begin{figure}[htb]
\centering
\includegraphics[width = 14.5cm]{img/slika3.png}
\caption{Prikaz epipolarnih linija u rektificiranim slikama skupa KITTI 2015}
\label{fig:Radial}
\end{figure}\\
Postupak se provodi kroz nekoliko koraka, a uključuje šahovsku ploču koja je pogodna zbog svojih svojstava. Prvi korak je traženje šahovskih polja. Postupak se nastavlja određivanjem izobličenja i ostalih parametara uz pomoć šahovskih polja.
Na slici 2.2 prikazan je par slika iz stereo kamera koji je rektificiran. Slike su iz skupa KITTI 2015, te se na njima vide epipolarne linije. Lako je primjetiti da iste točke u prostou nalaze na istoj visini, odnosno na istoj epipolarnoj liniji. Ovakvom transformacijom, korespodentne piksele potrebno je tražiti samo na istoj $y$-koordinati.

\section{Disparitet}
Prijašnjim postupcima kalibracije i rektifikacije omogućen je postupak računanja dispariteta, odnosno postupak računanja koji za svaki piksel koji je nastao lijevom kamerom određuje piksel slike koja je nastala desnom kamerom, ali pomaknutog za udaljenost $d$ piksela po horizonatlnoj osi. \textit{Disparitet} je definiran kao horizontalni pomak $d$ između dvaju korespodentnih piksela u slikama nastalim iz dviju konkurentnih stereo kamera. Kao rezultat računanja dispariteta nastaje mapa dispariteta koja sadrži disparitet za svaki piksel referentne kamere. Odnos piksela slike iz lijeve kamere, koja je u ovom slučaju referentna, i slike iz desne kamere definira se kao: 
\begin{equation}
I_{L}(x, y) = I_{D}(x-d,y)
\label{eq:Disparitet}
\end{equation}
gdje su $I_{L}$ i $I_{D}$ slike nastale lijevom, odnosno desnom kamerom, respektivno. Uređenim parom $(x,y)$ opisani su pikseli slike iz lijeve kamere, a njemu korespodentni pikseli u slikama iz desne kamere označeni su pomakom za disparitet $d$ po horizontalnoj osi $(x-d,y)$. Važno je napomenuti da su slike u skupu KITTI 2015 rektificirane i kalibrirane, što znači da korespodentne piksele tražimo samo po horizontalnoj osi.
Pomoću izračunatog dispariteta moguće je izraziti odnos između piksela kamera i dubine scene kao:
\begin{equation}
Z=f\frac{B}{d}
\label{eq:Fokalna}
\end{equation}
gdje je dubina scene, odnosno udaljenost objekta od kamere označena sa $Z$, $d$ je disparitet,s $B$ je označnena udaljenost između središta kamera, a s $f$ je označena fokalna udaljenost kamere. Iz jednadžbe 2.2 je vidljiva obrnuta proporcionalnost između udaljenosti objekta od kamera, te dispariteta s druge strane. Kada je položaj objekta blizu kamere, disparitet će biti velik. A kako se udaljavamo od kamere, disparitet se smanjuje i u beskonačnosti postiže vrijednost 0.\\
Na slici 2.4. je vidljivo da su epipolarne linije paralelne, te im je $y$-koordinata za korespodentne piksele jednaka. Disparitet je označen slovom $d$ i prikazuje horizontalni pomak.
\begin{figure}[htb]
\centering
\includegraphics[width = 14.5cm]{img/slika4.png}
\caption{Prikaz objekta u slikama iz lijeve i desne kamere. Plavom bojom označena je epipolarna ravnina. Okomitim osima označeni su projicirani pikseli u konkuretnim slikama. Disparitet je označen slovom $d$.}
\label{fig:Disparitet i epipolar}
\end{figure}\\
\\\\
\section{Algoritam stereoskopske rekonstrukcije}
Postupak stereoskopske korespodencije je zapravo postupak pronalaženja istih piksela u međusobno konkurentnim kamerama koji odgovaraju istoj točki u trodimenzionalnom prostoru. Postoje dvije vrste korespodencija, \textit{rijetke} i \textit{guste}. Razlika je u tome što rijetke ne izračunavaju disparitete za sve piksele, dok guste to rade. Guste korespodencije omogućene su današnjom snagom računala, te se rijetke više toliko ne koriste. Postupak guste korespodencije je tim teži što se prilikom izračuna susreće s nekim anomalijama u slikama, primjerice područjima bez teksture, reflesivnim podlogama na kojima se svjetlost drugačije lomi i odbija, te točkama koje se iz jedne kamere vide, a iz druge ne (\textit{stereoskopska sjena}).\\
Dolazimo do stereo algoritama, svaki od njih moguće je opisati u četiri koraka (\citep{scharszelinski}, \citep{kovac2015stereo}):
\begin{enumerate}
\item izračunavanje razlike između dvaju slikovnih okana, odnosno izračun podatkovnih cijena po prije definiranoj mjeri razlike
\item prikupljanje podatkovnih cijena za disparitete koji se razmatraju
\item izračun mape dispariteta, odnosno odabir najmanje prikupljene cijene za svaki piksel, uz optimizaciju
\item zaglađivanje mape dispariteta 
\end{enumerate}

Prvi korak ovog generičkog algoritma za stereoskopsku rekonstrukciju je izračun podatkovnih cijena. Izračun podatkovnih cijena može se računati prema pojedinačnim pikselima ili skupini piksela.  Izračun nam govori kolika je razlika između dvaju slikovnim okana, a razlika koja se koristi je definirana prije. Mjere razlike mogu biti sljedeće:
\begin{itemize}
\item[•] srednja kvadratna razlika
\item[•] srednja apsolutna razlika
\item[•] kvadratna razlika između intenziteta piksela
\item[•] apsolutna razlika između intenziteta piksela
\end{itemize}

\newpage
Drugi od koraka generičkog stereo algoritma je prikupljanje podatkovni cijena, tj. agregacija.  Na početku je postavljena granica za maksimalan disparitet, pa se tijekom ovogg koraka izračunava podatkovna cijena za sve disparitete i skupa  od $0$ do $D$, te ćemo tako za svaki piksel imati $D+1$ podatkovnih cijena.\\
Treći korak algoritma je izračun mape dispariteta. Nakon što su izračunate podatkovne cijene za razmatran skup dispariteta, treba odabrati metodu za izračun mape dispariteta. U praksi postoje \textit{lokalne} i \textit{globalne}. Lokalne metode prikupljaju cijene nad ograničenim područjem unutar same mape dispariteta, odnosno traže na ograničenom području oko referentne točke. Područje moje biti 2D ili 3D, te se prikupljanje može primjerice izvesti konvolucijom. S druge strane, globalne metode  preskaču korak prikupljanja podatkovnih cijena, te odmah nakon izračuna podatkovnih cijena vrše optimizaciju. Globalne metode minimiziraju kriterij nad cijelom slikom. Odnos između ovi metoda može se svesti na to da su lokalne brže, no globalne daju puno glađe mape dispariteta, što ih čini kvalitetnijima.\\
Zadnji korak koji je naveden je zaglađivanje mapa dispariteta. Ovaj korak je opcionalan i neke metode ga koriste. Glavni cilj ovog koraka je uklanjanje šuma koji je nastao generiranjem mape. 
\chapter{Duboko učenje}
Duboko učenje grana strojnog učenja koje se ubrzano razvija. U teoriji se dubokim učenjem smatra proces učenja neuronske mreže s 3 ili više slojeva, sve što je manje od toga naziva se \textit{plitkim} modelima. Cilj takvih neuronskih mreža je simulirati ponašanje ljudskog mozga omogućujući učenje na velikim skupovima podataka. Zahvaljujući razvoju grafičkih kartica, omogućen je ubrzan razvoj dubokog učenja.

U sljedećim odjeljcima bit će predstavljeni osnovni pojmovi i koncepti korišteni prilikom oblikovanja modela i procesa učenja.
\section{Osnovni pojmovi u dubokom učenju}
\subsection{Perceptron}
U ranim počecima duboko učenja došlo se do ideje potpuno povezanih modela. Jedan od takvih modela je višeslojni perceptron. Iako je danas napuštena ideja potpuno povezanih modela, zanimljivo je pogledati koje mane novi modeli imaju za ispraviti.
\begin{figure}[htb]
\centering
\includegraphics[width = 14.5cm]{img/slika5.png}
\caption{Jednostavni TLU perceptron}
\label{fig:Perceptron}
\end{figure}
\newpage
TLU perceptron su prvi definirali McCulloc i Pitts. Na slici 3.1. prikazan je jednostavni TLU perceptron. TLU perceptron ima više binarnih ulaza koji su označeni s $x_{1}$, $x_{2}$ i $x_{3}$, te proizvodi binarni izlaz. Rosenblatt je, kako bi odredio izlaz, definirao težine (u ovom slučaju $w_{1}$, $w_{2}$ i $w_{3}$). Težinama se mogu opisati doprinosi jednog neurona na drugi, odnosno koliko jedan utječe na drugog. Ovime je izlaz neurona definiran sumom umnoška težina i binarnih ulaza te praga s kojim s uspoređuje:
\begin{subequations}
\begin{empheq}[left=\ \empheqlbrace]{align}
0, {\sum_{k=1}^{n}} w_kx_k \le prag \\
1,  {\sum_{k=1}^{n}} w_kx_k > {prag}
\end{empheq}
\label{eq:Perceptron}
\end{subequations}
Jedan sloj neuronske mreže čine perceptroni koji se nalaze u istom stupcu. Izlaz svakog perceptrona  izračunava se tako koristeći izlaze prethodnog sloja i definirane težine za svaki od ulaznih perceptrona. Gornju formulu moguće je transformirati tako što se prag uvede nova varijabla $b=-prag$, koja označava pristranost (engl. \textit{bias}). Pristranost u kontekstu duboko učenja je moguće definirati koliko je lako natjerati neuron  da kao izlaz izbaci 1. Ukoliko je pristranost vrlo velik broj, izlaz e u veini slučajeva biti 1, dok je za vrlo negativan broj suprotna situacija. Izmijenjena formula glasi:
\begin{subequations}
\begin{empheq}[left=\ \empheqlbrace]{align}
0, {\sum_{k=1}^{n}} w_kx_k  + b\le 0 \\
1,  {\sum_{k=1}^{n}} w_kx_k + b > 0
\end{empheq}
\label{eq:Perceptron}
\end{subequations}
Linearnu funkciju koju smo izveli možemo predstaviti i u matričnom obliku:
\begin{equation}
f=w^{T}x+b
\label{eq:Linear}
\end{equation}
\subsection{Prijenosne funkcije}
Ukoliko bi koristili linearnu funkciju izlaza iz prijašnjeg odjeljka, mala promjena u ulazu bi na izlazu generirala veliku promjenu. Stoga se uvode prijenosne, odnosno aktivacijske funkcije jer odlučuju hoće li neki neuron biti aktivan ili ne. Primarni cilj prijenosnih funkcija je omogućiti da se primijete sitne promjene na ulazu. Aktivaciju neurona možemo predstaviti sljedećom formulom:
\begin{equation}
a^k=\sigma_{k}(W^T_{k}a^{k-1}+b)
\label{eq:Aktivacija}
\end{equation}
gdje $a^k$ označava aktivaciju neurona trenutnog sloja, $a^{k-1}$ aktivaciju neurona prijašnjeg sloja, dok $\sigma$ označava aktivacijsku funkciju.

Prva funkcija koja će biti objašnjena je sigmoidalna funkcija čiji izraz je dan u nastavku:
\begin{equation}
\sigma(x)=\frac{1}{1 + e^{-x}}
\label{eq:Aktivacija}
\end{equation}
Glavno obilježje sigmoidalne funkcije je to što je njezina kodomena na skupu od 0 do 1, te ju to čini primjenjivom u regresijskim i klasifikacijskim problemima. Veliki pozitivni brojevi poprimit će vrijednost 1, dok će jako negativni brojevi poprimiti vrijednost 0. Također, prednost je što je derivabilna, pa omogućuje učenje u kojem se koristi gradijentni spust. Glavni nedostatak zbog kojeg se sve manje koristi je problem nestanja gradijenata (engl. \textit{vanishing gradient problem}) koji govori da gradijenti postaju toliko mali da ne nose nikakvu informaciju koja bi se mogla iskoristiti za učenje. Prikaz grafa funkcije je na slici 3.2.\\
\begin{figure}[htb]
\centering
\includegraphics[scale=0.6]{img/slika6.png}
\caption{Graf sigmoidalne funkcije}
\label{fig:Perceptron}
\end{figure}
\linebreak
Funkcija koja je usko vezana uz sigmoidalnu je \textit{tangens hiperbolni}. Kodomena ove funkcije je na intervalu od -1 do 1. Problem nestajanja gradijenata je isto tako prisutan. Kako je bliska sigmoidalnoj, možemo uspostaviti odnos između njih koji je oblika:
\begin{equation}
\tanh(x) = 2\sigma(2x)-1
\label{eq:Aktivacija}
\end{equation}
\begin{figure}[htb]
\centering
\includegraphics[scale=0.6]{img/slika7.png}
\caption{Graf funkcije tangensa hiperbolnog}
\label{fig:Perceptron}
\end{figure}\\
\\\\\\\\
Sljedeća prijenosna funkcija koja se koristi je zglobnica (engl. \textit{ReLU - rectified linear unit}). Ova prijenosna funkcija propušta pozitivne vrijednosti, dok negativne vrijednosti ne propušta. Glavna prednost ove funkcije što je računski izuzetno jeftina, dok joj je nedostatak što za negativne vrijednosti uvijek daje 0. Izraz koji ju opisuje je oblika:
\begin{equation}
f(x)=max(0,x)
\label{eq:Aktivacija}
\end{equation}
\begin{figure}[htb]
\centering
\includegraphics[scale=0.6]{img/slika8.png}
\caption{Graf funkcije ReLU}
\label{fig:Perceptron}
\end{figure}
\linebreak
\\\\
Moguće je napraviti modifikaciju zglobnice tako da za negativne vrijednosti daje vrlo male vrijednosti, te se takva prijenosna funkcija naziva propusnom zglobnicom (engl. \textit{LReLU - leaky rectified linear unit}). U nastavku slijedi izraz koji ju opisuje:
\begin{subequations}
\begin{empheq}[left={f(x)=}\ \empheqlbrace]{align}
x, x > 0 \\
\alpha x,  x \le 0
\end{empheq}
\label{eq:Perceptron}
\end{subequations}
gdje je $\alpha$ parametar, odnosno u praksi vrlo mali pozitivan broj. Na slici 3.5. je vidljivo da negativne vrijednosti propušta uz prigušenje, dok na slici 3.4. obična zglobnica ne propušta.
\begin{figure}[htb]
\centering
\includegraphics[scale=0.6]{img/slika9.png}
\caption{Graf funkcije LReLU}
\label{fig:Perceptron}
\end{figure}
\subsection{Konvolucijski slojevi}
Konvolucijski slojevi osnovni su gradivni element konvolucijskih neuronskih mreža. Kako je u radu riječ o slikama, bit će predstavljena $2D$ konvolucija, operacija filtriranja. Uloga filtriranja je uočavanje i učenje uzoraka koji se nalaze na slikama. 
Konvolucijski slojevi čine konvolucije ulaza s različitim filtrima, drugi naziv je jezgra (engl. \textit{kernel}).Izlaz iz 2D konvolucije je aktivacijska mapa, odnosno mapa značajki, dok je izlaz cijelog konvolucijskog sloja više mapa značajki. Kako dubina tijekom prolaska kroz mrežu raste, identificiraju se sve specifičniji uzorci. Tijekom unaprijednog prolaza, jezgra se provlači po matričnoj reprezentaciji slike i računa se skalarni umnožak između jezgre i ulaza na svakom položaju. 
\\\\\\
\begin{equation}
\begin{bmatrix}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23} \\ 
x_{31} & x_{32} & x_{33}
\end{bmatrix}*\begin{bmatrix}
w_{11} & w_{12} \\
w_{21} & w_{22}
\end{bmatrix}=
\begin{bmatrix}
x_{11}w_{11} + x_{12}w_{12} & x_{12}w_{11} + x_{13}w_{12}\\+ x_{21}w_{21} + x_{22}w_{22} &   + x_{22}w_{21} + x_{23}w_{22}\\ 
\\
x_{21}w_{11} + x_{22}w_{12} & x_{22}w_{11} + x_{23}w_{12}\\+ x_{31}w_{21} + x_{32}w_{22} &  +x_{32}w_{21} + x_{33}w_{22}
\end{bmatrix}
\label{eq:Aktivacija}
\end{equation}
\linebreak
Jednadžbom 3.9 prikazana je matrična reprezentacija dimenzija $3x3$, dok je jezgra dimenzije $2x2$. Jezgra se provlači po ulaznoj matrici (računa se skalarni umnožak) i nastaje izlazna mapa značajki dimenzija $2x2$. Iz ovog je vidljivo da se konvolucijom smanjuje dimenzija ulaza.

Ukoliko želimo zadržati rezoluciju slike nakon konvolucijskog sloja, potrebno je uvesti nadopunjavanje (engl. \textit{padding}). Najčešće je korištena metoda popunjavanja nulama. Još jedan od hiperparametara je korak (engl. \textit{stride}). Korak govori o tome koliko se jezgra pomiče u vertikalnom i horizontalnom smjeru. U jednadžbi 3.9 je korišten korak 1.
\section{Učenje dubokih modela}
Učenje dubokih modela, u generalnom slučaju neuronske mreže, je postupak tijekom koje se modelu na ulaz daju podaci iz skupa za učenje. Tijekom procesa učenja model ažurira težine u slojevima s ciljem kako bi što bolje aproksimirao izlaznu funkciju. Za početak kako bi mogli odrediti koliko odstupamo od pravih vrijednosti, uvodimo pojam funkcije gubitka. U procesu učenja provodi se minimizacija funkcije gubitka. Ukoliko na skupu podataka gubitak ispada malen, a na skupu podataka koje još nije vidio ispada izražen, govorimo o tzv. prenaučenosti modela, što znači da model nema sposobnost generalizacije.\\
Postoji nekoliko pristupa učenju, nadzirano učenje u kojem se modelu predaju parovi oblika (ulaz, željeni izlaz), nenadzirano u kojem se modelu samo predaju ulazni podaci, te podržano učenje u kojem se maksimizira kumulativna nagrada. U ovom radu bit će razmatrano nadzirano učenje.\pagebreak
\subsection{Funkcija gubitka}
Kako bismo procijenili koliko model odstupa od željenih vrijednosti izlaza koristimo funkciju gubitka. Ako model loše procjenjuje, gubitak će biti velik. 
Kod problema klasifikacije najčešće je korištena unakrsna entropija (engl.\textit{cross-entropy loss}):
\begin{equation}
L_{CE}=-\sum_{i=1}^{n}t_{i}\cdot log(p_{i})
\label{eq:Aktivacija}
\end{equation}
Popularan klasifikator koji koristi unakrsnu entropiju, a daje vjerojatnosnu reprezentaciju izlaza modela je funkcija \textit{softmax}:
\begin{equation}
f_{j}(z) = \frac{e^{{z_{j}}}}{\sum_{k}^{}e^{z_{k}}}
\label{eq:Aktivacija}
\end{equation}
\subsection{Gradijentni spust}
U prijašnjem pododjeljku definirana je funkcija gubitka, cilj učenja modela je minimizirati gubitak. Kako bi se parametri pomicali u smjeru minimuma funkcije gubitka, a da istovremeno pomak bude što veći izračunava se gradijent funkcije gubitka. \\
Neka je zadana funkcija gubitka koja ovisi o težinama $\textbf{W}$ i pomacima $\textbf{b}$. Gradijenti funkcije gubitka mogu se zapisati kao:
\begin{equation}
\nabla L_{w} = \left(\frac{\partial L}{\partial w_{1}},...,\frac{\partial L}{\partial w_{n}} \right)
\label{eq:Aktivacija}
\end{equation}
\begin{equation}
\nabla L_{b} = \left(\frac{\partial L}{\partial b_{1}},...,\frac{\partial L}{\partial b_{n}} \right)
\label{eq:Aktivacija}
\end{equation}
gdje $n$ predstavlja broj parametara.\\
U iteracijama gradijentnog spusta parametri se ažuriraju prema sljedećim izrazima:
\begin{equation}
W_{i+1} = W_{i} - \epsilon \cdot \nabla L_{w}(\textbf{W},\textbf{b})
\label{eq:Aktivacija}
\end{equation}
\begin{equation}
b_{i+1} = b_{i} - \epsilon \cdot \nabla L_{b}(\textbf{W},\textbf{b})
\label{eq:Aktivacija}
\end{equation}
gdje je $\epsilon$ tzv. stopa učenja (engl. \textit{learning rate}), a predstavljena je malim pozitivnim realnim brojem.\cite{duboko} \pagebreak
\subsection{Optimizacija}
Kad govorimo o optimizaciji, najkorištenija metoda je svakako gradijentni spust, no korištenjem njega se mogu manifestirati neki problemi koji kao posljedicu imaju nepronalazak optimalnih težina. Postojanje više lokalnih minimuma i spora kovergencija jedni su od problema. 
U ovom radu je korišten ADAM optimizator,a svakako potrebno je spomenuti i SGD optimizator. 

Glavna ideja optimizatora SGD je procijeniti gradijent cijelog skupa uzoraka sa malim slučajno odabranim podskupom uzoraka (mini-grupa, engl. \textit{minibatch}). Već i veličine od 128 daju vrlo dobru aproksimaciju gradijenta. U usporedbi s "običnim" gradijentnim spustom (GD) to znatno ubrzava proces učenja, jer se kod GD izračun treba provesti za cijeli skup. Stohastički gradijentni spust koristi se uz zalet koji ubrzava učenje. Umjesto ažuriranja težina koristeći samo trenutno izačunat gradijent, zalet također uzima u obzir i prethodno ažuriranje, pomnoženo sa faktorom umanjivanja $\alpha$.\cite{sgd}\\ 
Modifikacija u odnosu na GD je vidljiva sljedećom jednadžbom:
\begin{align}
v = \mu v + \nabla L_{w}
\end{align}
,gdje je $v$ trenutno akumuliran zalet, a $\mu$ je zalet.

Adaptive Moment Estimation (ADAM) je optimizator koji kombinacijom dvije metodologije gradijentnog spusta zaleta i RMSprop algoritma ubrzava pronalazak globalnog minimuma. Glavna značajka ADAM-a je što koristi prosječno eksponencijalno kretanje gradijenta i kvadratnog gradijenta, te ga to čini izrazito efikasnim na velikim skupovima podataka i u radu s velikim brojem parametara\cite{adam}.
\subsection{Regularizacija}
Kako bi se izbjegla prenaučenost modela (engl. overfitting), odnosno loša generalizacija modela potrebno je uvesti regularizatore koji imaju za cilj spriječiti prenaučenost.\\
Jedna od metoda koje se koriste je regularizacija dodavanjem regularizacijskog faktora funkciji gubitka. Pri tome treba spomenuti $L1$ i $L2$ regularizaciju, kojima se kažnjava norma vektora težina.
\begin{equation}
L_{R}= L + \lambda \cdot \sum_{i} |w_{i}|
\label{eq:Aktivacija}
\end{equation}
\begin{equation}
L_{R}= L + \lambda \cdot \sum_{i} w_{i}^2
\label{eq:Aktivacija}
\end{equation}
Izrazom 3.17 opisana je $L1$ regularizacija, a 3.18 $L2$ regularizacija.\\ \\
Regularizacijska metoda koja istovremeno ima i ulogu optimizacije je normalizacija grupe (engl. \textit{batch norm}). Normalizacija grupe primjenjuje se prije nelinearnosti, a temelji se na normalizaciji izlaza na normalnu jediničnu razdiobu. Posljedica takvog pristupa je što ulazi u slojeve imaju sličnu razdiobu, čime se postiže ubrzanje učenja bržom propagacijom gradijenata.
U postupku normalizacije u mini grupi za svaku izlaznu mapu značajki računaju se srednja vrijednost i varijanca po sljedećim izrazima:
\begin{equation}
\mu _{B} = \frac{1}{m}\sum_{i=1}^{m}\textbf{x}_{i}
\label{eq:Aktivacija}
\end{equation}
\begin{equation}
\sigma^{2} _{B} = \frac{1}{m}\sum_{i=1}^{m}\left(\textbf{x}_{i} - \mu_{B} \right)
\label{eq:Aktivacija}
\end{equation}
gdje $B$ označava mini grupu, a $x$ vrijednost ulaza.
Nelinearnost se onda primijenjuje na tako normaliziranom izlazu.
\chapter{Podatkovni skup za učenje}
Kako je već rečeno, u ovom radu učenje je nadzirano. Podatkovni skup KITTI jedan je od skupova koji se koriste u učenju postupka stereoskopske rekonstrukcije.U sljedećem pododjeljku bit će predstavljen podatkovni skup KITTI 2015, njegove specifičnosti, te postupci pripreme podataka.
\section{Podatkovni skup KITTI 2015}
Podatkovni skup KITTI 2015, skup je od 200 stereo parova slika za koje su laserski izmjerene mape dispariteta. Stereo par slika je unaprijed kalibriran i rektificiran. Primjer uz odgovarajuće disparitete prikazan je na slici 4.1. Podatkovni skup je često korišten u postupcima stereoskopske rekonstrukcije za autonomnu vožnju.Podatkovni skup je podijeljen na skup za učenje kojeg čini 80\% slika, sljedećih 20\% čini skup za validaciju.\\
Mape dispariteta su vrlo rijetke, odnosno dispariteti su izmjereni laserski za manji podskup piksela. Takve mape će na mjestima piksela za koje nije izmjeren disparitet imati vrijednost nula. Primjer takvih piksela su oni koji prikazuju udaljene objekte, kao što je nebo. Takvo mjerenje proizlazi iz toga što je disparitet obrnuto proporcionalan udaljenosti, te poprima vrijednost nula u beskonačnosti. \\
Također napravljena je intervencija na područjima slike na kojima se nalazi automobil. Intervencija se sastojala od progušćivanja dispariteta na mjestima automobila. Ova intervencija je od velike važnosti rekostrukcijskim algoritmima jer se na automobilu nalaze refleksivne površine poput stakla koje u konkurentnim stereo kamerama mogu izgledati drugačije.\cite{zbontar}
\begin{figure}[htb]
\centering
\includegraphics[width = 14.5cm]{img/slika11.png}
\caption{Primjer iz podatkovnog skupa KITTI 2015. Gornja slika prikazuje referentnu lijevu sliku, a donja slika prikazuje disparitet za odgovarajuću sliku.}
\label{fig:KITTI}
\end{figure}
\chapter{Duboko učenje u kontekstu stereoskopske rekonstrukcije}
U ovom poglavlju bit će predstavljena arhitektura dubokog modela koji se koristi za stereoskopsku rekonstrukciju. Metoda koristi ugrađivanje okana u visokodimenzionalan metrički prostor, pri čemu je moguće uspoređivanje po sličnosti koje koristimo u traženju korespodentnih piksela.
\section{Ugrađivanje okana u metrički prostor}
Stereo algoritmi zahtijevaju robusne metrike s obzirom na šum, dok s druge strane tražimo da metrika na pikselima bez teksture nosi određenu informaciju. U ovom radu razmatran duboki model koji uči na slikama s poznatim disparitetima zadovoljava uvjete stereo algoritama.\\
U ovom radu se koristi duboki model za ugrađivanje slika u visokodimenzionalan prostor.\citep{zbontar2} Duboki model sastoji se od 4 konvolucijska sloja, pri čemu svaki sloj ima 64 značajke (engl. features). Za konvoluciju je korištena jezgra veličine $3 x 3$. Na ulaz modela dovodimo slikovno okno koje je dimenzija $P x P$, dok je izlaz modela ugrađivanje okna u 64 dimenzionalan vektor. Ono što konvolucijskoj arhitekturi daje prednost je što je moguće ugrađivanje provesti u jednom prolazu nad cijelom slikom. Prilikom takvog ugrađivanja koristi se nadopunjavanje nulama da se dobije izlaz istih prvih dviju dimenzija,nadalje ulaz je slika dimenzija $W x H$, a izlaz ugrađivanje čije su dimenzije $W x H x 64$. Kako bismo mogli koristi kosinusnu sličnost (objašnjeno kasnije), potrebno je normirati izlaze na jedinični vektor duž osi značajki.\pagebreak
\\
Nakon što je dubokim modelom ostvareno ugrađivanje okana u visokodimenzionalan prostor, moguće je provesti izračun mape dispariteta. Razmatramo jedan par slika $I_{L}$ i $I_{D}$. Provedenim ugrađivanjem nastaju $E_{L}$ i $E_{D}$ spomenutih dimenzija $W x H x 64$. Nad takvim reprezentacijama tražimo korespodentne piksele. Kako je već spomenuto slike su rektificirane, što nam omogućuje usporedbu samo po horizontalnoj ulici. U ovom radu razmatrat će se dispariteti u intervalu $[0...D]$, pri čemu je $D$ maksimalan disparitet u skupu KITTI i iznosi 229. Kako je lijeva slika referentna, desna ugrađivanja posmičemo za disparitete u razmatranom intervalu uzduž horizontalne osi. Nakon posmaka vrši se skalarni umnožak vektora na odgovarajućim indeksima. Takvim množenjem dobiva se vektor dimenzija $W x H x D$, na čijim trećim dimenzijama se nalaze mjere sličnosti piksela.\\
Opisana mjera sličnosti vektora naziva se \textit{kosinusna sličnost vektora}. U nastavku je dan matematički izraz takve mjere sličnosti:
\begin{equation}
\frac{\textbf{a}\cdot\textbf{b}}{\parallel\textbf{a}\parallel \parallel\textbf{b}\parallel}=\frac{\sum_{i}a_{i}b_{i}}{(\sum_{i}a_{i}^2)(\sum_{i}b_{i}^2)}
\label{eq:normalizacija}
\end{equation}
Kako je cilj pronaći najsličnije piksele koristi se operacija $argmax$ po trećoj dimenziji. Funkcija $argmax$ će vratiti indeks najsličnijeg piksela, odnosno čija je mjera sličnosti najveća. Provedenim operacijama je nastala tzv. mapa dispariteta. Kako se u postupku ne uzima u obzir susjedstvo piksela, ovakav pristup se naziva i \textit{Winner takes all} zato što se razmatraju samo najsličniji pikseli. Na slici 5.1. prikazan je postupak izračunavanje mape dispariteta.
\begin{figure}[htb]
\centering
\includegraphics[width = 14.5cm]{img/disp_map.png}
\caption{Postupak računanja mape dispariteta. Stereo par slika iz konkurentnih kamera ide na ulaz, prolazi kroz 4 kovolucijska sloja, te se na kraju izlaz normira na jedinični vektor. U zadnjem koraku se pronalaze nasličniji pikseli metodom \textit{Winner takes all}.}
\label{fig:KITTI}
\end{figure}
\pagebreak
\section{Učenje modela za ostvarivanje korespodencije}
Prije no što se krene s učenjem potrebno je svesti piksele slika na normalnu jediničnu razdiobu. Za takvo što je potrebno izračunati srednju vrijednost $\mu$ i standardna devijacija $\sigma$, te je konačan izraz za piksele $I(x,y)$:
\begin{equation}
I'(x,y)= \frac{I(x,y) - \mu}{\sigma}
\label{eq:normalizacija}
\end{equation}
Nakon što je izvršena normalizacija izdvajaju se okna nad kojima će spomenuti duboki model učiti.
Ukupan skup sastoji se od oko 16 milijuna primjera, a za svaki od njih poznat je disparitet.
Primjer za učenje sastoji od tri okna, a to su referentno okno R, pozitivno okno P i negativno okno N. Referentno i pozitivno okno čine međusobno sličan par, dok referetno i negativno čine manje sličan par. U nastavku je objašnjena ekstrakcija okna:
\begin{itemize}
\item[•] $R = I_{L}^{PxP}(x,y)$ je okno lijeve slike i predstavlja referentno okno
\item[•] $P = I_{D}^{PxP}(x-d,y)$ je pozitivno okno, a dobiva se horizontalnim posmakom središta za poznati disparitet $d$
\item[•] $P = I_{D}^{PxP}(x-d-o,y)$ je negativno okno, a dobiva se horizontalnim posmakom središta  za disparitet $d$ te dodatnim nasumičnim odmakom $o$. Odmak se odabire iz intervala $[-14, -4] U [4, 14]$.
\end{itemize}
Na slici 6.2. prikazano je nekoliko primjera za učenje.

\begin{figure}[htb]
\centering
\includegraphics[scale = 0.7]{img/okna.png}
\caption{Prikaz 4 primjera za učenje. Prvi redak predstavlja referentno lijevo okno, redak ispod prikazuje čvrsti pozitiv (pozitivno okno bez nasumičnog pomaka), te zadnji redak prikazuje negativno okno.}
\label{fig:KITTI}
\end{figure}

\pagebreak
Tijekom postupka optimizacije minimizira se funkcija gubitka čiji je izraz dan u nastavku:
\begin{equation}
L(R, P, N) = max(0, m + \textbf{h}(\textbf{R}|\theta)\cdot \textbf{h}(\textbf{N}|\theta)-\textbf{h}(\textbf{R}|\theta)\cdot \textbf{h}(\textbf{P}|\theta)
\label{eq:gubitak}
\end{equation}
pri čemu $h$ predstavlja izlaz modela s parametrima $\theta$. U funkciji gubitka zahtijevano je da je ugrađivanje referentnog okna sličnije ugrađivanju pozitivnog okna u odnosu na ugrađivanje negativnog okna za najmanje $m$. Pojednostavljeno rečeno, model će učiti samo na primjerima na kojima je sličnost $R$ i $P$ manja od sličnosti između $R$ i $N$ za više od $m$.\cite{orsic}

\chapter{Eksperimentalni rezultati}
Kvalitetu rekonstrukcijske točnosti ocjenuje se na slikama za koje su poznati dispariteti. Svi eksperimenti provedeni su na predstavljenom skupu KITTI. Za točno rekonstuirani piksel smatrat će se ako je odstupanje od točnog manje od 3. U tu svrhu koristit će se skup za testiranje koji se sastoji od 20\% slika početnog skupa od 200 slika.\\
Učenje traje 14 epoha, algoritam učenja je ADAM uz stopu učenja koja iznosi $1e^{-3}$ do 10. epohe, a nakon nje je $1e^{-4}$. Veličina mini-grupe (engl. batch-size) je 128.  Svi eksperimenti izvodili su se na platformi Colab pro, a učenje je trajalo 9 sati.\\
Provedena su sljedeća 3 eksperimenta:
\begin{itemize}
\item[•]Ulazne slike u boji (RGB) s čvrstim pozitivima, odnosno bez posmaka
\item[•]Ulazne slike u boji (RGB) s rastresenim pozitivima, odnosno nasumično \linebreak posmaknutim brojem iz skupa \{-1, 0, 1\}
\item[•]Sive ulazne slike s čvrstim pozitivima
\end{itemize}

\begin{table}[htb]
\caption{Rekonstrukcijske točnosti razmatrani modela}
\label{tbl:konstante}
\centering
\begin{tabular}{llr} \hline
Model & Treniranje & Testiranje\\ \hline
Ulazne slike u boji (RGB) s čvrstim pozitivima & 82.90\% & 81.24\% \\
Ulazne slike u boji (RGB) s rastresenim pozitivima & 82.43\% & 80.81\% \\
Sive ulazne slike s čvrstim pozitivima &  &  \\ \hline
\end{tabular}
\end{table}
U tablici 6.1. prikazane su rekonstrukcijske točnosti razmatranih eksperimenata. Najbolji od 3 razmatrana modela je onaj koji koristi ulazne slike u boji s čvrstim pozitivima.\pagebreak

Na slikama 6.1, 6.2 i 6.3 prikazano je kretanje prosječnih gubitaka po epohama i to na skupovima za treniranje i validaciju. Slike prikazuju grafove za model s čvrstim i rastresenim pozitivima za RGB, te model s čvrstim  pozitivima za sive ulazne slike. Vidljivo je kako model koji koristi čvrste pozitive brže minimizira pogrešku. Što se tiče sivih slika s čvrstim pozitivima u odnosu na RGB slike s čvrstim pozitivima,  model s RGB slikama je bolji, razlog leži u tome što RGB slike imaju 3 kanala,te posljedično nose više informacija.
\begin{figure}[htb]
\centering
\includegraphics[scale = 0.44]{img/HARDPOS_RGB.png}
\caption{Graf prikazuje kretanje prosječnog gubitka tijekom epoha treniranja i validacije. Za učenje modela se koriste čvrsti pozitivi RGB slika, odnosno bez posmaka.}
\label{fig:KITTI}
\end{figure}
\begin{figure}[htb]
\centering
\includegraphics[scale = 0.44]{img/LOOSE_RGB.png}
\caption{Graf prikazuje kretanje prosječnog gubitka tijekom epoha treniranja i validacije. Za učenje modela se koriste rastreseni pozitivi RGB slika (posmaknuti za nasumičan broj u skupu \{-1, 0, 1\}).}
\label{fig:KITTI}
\end{figure}
\begin{figure}[htb]
\centering
\includegraphics[scale = 0.44]{img/LOOSE_RGB.png}
\caption{Graf prikazuje kretanje prosječnog gubitka tijekom epoha treniranja i validacije. Za učenje modela se koriste čvrsti pozitivi sivih slika.}
\label{fig:KITTI}
\end{figure}
\pagebreak\\
Na slici 6.4. prikazane su rekonstrukcije za 4 slike nasumično odabrane iz skupa. Na mapama dispariteta je vidljiva velika razlika u susjednim pikselima, što je posljedica nekorištenja nekog od filtera zaglađivanja.  Razmatrana metoda griješi na mjestima slike na kojima se nalaze refleksivne površine automobila. Na takvim područjima se svjetlost odbija, te okna iz tih područja predstavljaju problem modelu u procesu učenja. Sljedeća pojava koja predstavlja problem modelu su područja koja se na slikama jedne kamere vide, a na slikama druge kamere ne. Ta pojava naziva se stereoskopskom sjenom.\cite{ilinovic}

\begin{figure}[htb]
\centering
\includegraphics[scale = 0.43]{img/eval.png}
\caption{Prikaz rekonstrukcijske točnosti modela za ugrađivanje okana u visokodimenzionalan prostor. Lijeve slike prikazuju slike iz lijeve referentne kamere. U sredini se nalaze procjenjene mape dispariteta. Na mapi dispariteta što je svjetliji piksel to je veći disparitet, a tamniji što je manji.Zdesna se nalaze rekonstrukcijske točnosti, pri čemu su crnom bojom označeni pikseli uz nepoznat disparitet, plavom bojom pogrešno rekonstruirani pikseli, te zelenom bojom točno rekonstruirani pikseli.}
\label{fig:KITTI}
\end{figure}
\chapter{Programska izvedba i vanjske biblioteke}
Za izradu ovog rada korišten je programski jezik Python. Za izgradnju  i učenje modela korišteno je radno okruženje PyTorch. Ovo okruženje nudi velik spektar alata i podrške, a jedan od njih je automatska diferencijacija, što uvelike ubrzava proces izgradnje i učenja modela. Torch kao sastavna jednica PyTorcha omogućava izračune tenzorima na velikoj paleti grafičkih kartica kompanije Nvidia.\\ Za pripremu podataka i računanje tenzorima korištena je biblioteka Numpy. Ova biblioteka pisana je u jeziku niske razine C, što čini operacije vrlo brzim.\\
Za proces učenja, validacije i testiranja korištena je platforma Colab Pro u kombinaciji s Google Drive. Platforma u pretplati Pro nudi 3 grafičke kartice koje se dodjeljuju ovisno o raspoloživosti. Tipovi grafičkih kartica su sljedeći: K80, T4 i P100.\\
Podatkovni skup KITTI 2015 uzet je s javno dostupne stranice u vlasništvu kolaboracije Karlsruhe Institute of Technology i Toyota Technological Institute. 
\chapter{Zaključak}

\bibliographystyle{fer}
\bibliography{literatura}


\begin{sazetak}
Sažetak na hrvatskom jeziku.

\kljucnerijeci{Ključne riječi, odvojene zarezima.}
\end{sazetak}

% TODO: Navedite naslov na engleskom jeziku.
\engtitle{Title}
\begin{abstract}
Abstract.

\keywords{Keywords.}
\end{abstract}

\end{document}
