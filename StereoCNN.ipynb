{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StereoCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OVqTSOlQbfY7TP5TqC-FQgPNfNgB9ax6",
      "authorship_tag": "ABX9TyPGuLfX7bECNnf8K7nGH5fm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidenko2000/stereo-matching-CNN/blob/main/StereoCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATCH_SIZE = 9\n",
        "MAX_DISP = 192\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "PIXEL_ERROR = 3\n",
        "\n",
        "LR = 0.001\n",
        "LR_CHANGE_AT_EPOCH = 11\n",
        "LR_AFTER_10_EPOCHS = 0.0001\n",
        "\n",
        "\n",
        "FEATURES = 64\n",
        "MARGIN = 0.2\n",
        "EPOCHS = 14"
      ],
      "metadata": {
        "id": "QHJygVNFSw7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGES_DIR = 'drive/MyDrive/data_scene_flow/'\n",
        "\n",
        "DISP_DIR = IMAGES_DIR + 'disparity/'\n",
        "RGB_DIR = IMAGES_DIR + 'RGB/'\n",
        "GRAY_DIR = IMAGES_DIR + 'GRAY/'\n",
        "\n",
        "RGB_LEFT_DIR = RGB_DIR + 'left/'\n",
        "RGB_RIGHT_DIR = RGB_DIR + 'right/'\n",
        "GRAY_LEFT_DIR = GRAY_DIR + 'left/'\n",
        "GRAY_RIGHT_DIR = GRAY_DIR + 'right/'\n",
        "\n",
        "IS_GRAY = False\n",
        "BNORM = False\n",
        "\n",
        "PATCH_SIZE = 9\n",
        "MAX_DISPARITY = 100 #povecat mozda\n",
        "\n",
        "NUM_IMAGES = 200\n",
        "TRAIN_START = 0\n",
        "TRAIN_END = 160\n",
        "VALID_START = 160\n",
        "VALID_END = 200\n",
        "\n",
        "TRAIN_DATA = './train/'\n",
        "VALID_DATA = './valid/'"
      ],
      "metadata": {
        "id": "V5ij5W3fSo8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import skimage\n",
        "import torch\n",
        "from matplotlib import image as mpimg\n",
        "from torch import nn\n",
        "import torchvision.datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "#Method returns a filename which is made of prefix, which length is 6\n",
        "def get_filename(idx):\n",
        "      return str.zfill(str(idx), 6) + \"_10.png\"\n",
        "\n",
        "# Method returns an disparity image at index (HxW)\n",
        "def get_disp_image(idx):\n",
        "      return skimage.util.img_as_ubyte(mpimg.imread(DISP_DIR + get_filename(idx)))\n",
        "\n",
        "# Method returns an image at index (HxWxC)\n",
        "def get_image(idx, is_left):\n",
        "    \treturn mpimg.imread((GRAY_LEFT_DIR if IS_GRAY else RGB_LEFT_DIR) + get_filename(idx)) if is_left else mpimg.imread((GRAY_RIGHT_DIR if IS_GRAY else RGB_RIGHT_DIR) + get_filename(idx))\n",
        "  \n",
        "#Method which computes a disparity map of the left and right image\n",
        "def compute_disparity_map(idx, model):\n",
        "      max_disp = MAX_DISP\n",
        "      model.to('cuda').eval()\n",
        "\n",
        "      left_img = get_image(idx, is_left=True)\n",
        "      right_img = get_image(idx, is_left=False)\n",
        "\n",
        "      #Adding the padding, as a result the output image will have the same dimensions as input\n",
        "      padding = PATCH_SIZE // 2\n",
        "      left_pad = transforms.ToTensor(np.pad(left_img, ((padding, padding),(padding, padding)))).unsqueeze(0)\n",
        "      right_pad = transforms.ToTensor(np.pad(right_img, ((padding, padding),(padding, padding)))).unsqueeze(0)\n",
        "\n",
        "      #Making a ndarray of the tensor, which is the output of the model\n",
        "      left_array = left_pad.squeeze(0).permute(1, 2, 0).detach().numpy()\n",
        "      right_array = right_pad.squeeze(0).permute(1, 2, 0).detach().numpy()\n",
        "\n",
        "      #ndarray[HxWxD]\n",
        "      stacked_disp =  np.stack([np.sum(left_array * np.roll(right_array, d, axis=1) , axis=2) for d in range(max_disp)], axis=2)\n",
        "      #ndarray[HxW]-using argmax to extract the most similar\n",
        "      disp_map =  np.argmax(stacked_disp, axis=2)\n",
        "\n",
        "      return disp_map\n",
        "\n",
        "#Method which computes mean and standard deviation, used for normalization\n",
        "def get_mean_std():\n",
        "      RGB_transform = transforms.Compose([\n",
        "              transforms.Resize(256),\n",
        "              transforms.CenterCrop(256),\n",
        "              transforms.ToTensor()\n",
        "          ])\n",
        "      GRAY_transform = transforms.Compose([\n",
        "          transforms.Resize(256),\n",
        "          transforms.CenterCrop(256),\n",
        "          transforms.Grayscale(),\n",
        "          transforms.ToTensor()\n",
        "      ])\n",
        "\n",
        "      images_data = torchvision.datasets.ImageFolder(root=(GRAY_DIR if IS_GRAY else RGB_DIR),\n",
        "                transform=(GRAY_transform if IS_GRAY else RGB_transform))\n",
        "      data_loader = DataLoader(images_data, batch_size=len(images_data), shuffle=False, num_workers=0)\n",
        "      images, _ = next(iter(data_loader))\n",
        "      mean, std = images.mean([0, 2, 3]), images.std([0, 2, 3])\n",
        "      return mean, std\n",
        "\n",
        "#Method returns disparity file, which consists of narrays\n",
        "def load_disparity_data(train=True):\n",
        "    \treturn np.load((TRAIN_DATA if train else VALID_DATA) + 'disparities.npy')\n",
        "\n",
        "#Method which converts RGB images to grayscale\n",
        "def convert_to_grayscale():\n",
        "      for idx in range(NUM_IMAGES):\n",
        "            filename = get_filename(idx)\n",
        "            Image.open(RGB_LEFT_DIR + filename).convert(\"L\").save(GRAY_LEFT_DIR + filename)\n",
        "            Image.open(RGB_RIGHT_DIR + filename).convert(\"L\").save(GRAY_RIGHT_DIR + filename)\n",
        "\n",
        "#Method which makes narrays and saves disparity data to file\n",
        "def make_disparity_data(train=True):\n",
        "      distance = PATCH_SIZE // 2\n",
        "      def filter(idx):\n",
        "              disp_image = get_disp_image(idx)\n",
        "              rows, cols = disp_image.nonzero() #returns non zero pixels (pixels with known disparity)\n",
        "              rows = rows.astype(np.uint16)\n",
        "              cols = cols.astype(np.uint16) \n",
        "              disparity_values = disp_image[rows, cols]\n",
        "              \n",
        "              pos_cols = cols - disparity_values #computes cols with correct disparity\n",
        "              neg_cols = pos_cols + np.random.choice([-8, -7, -6, -5, -4, 4, 5, 6, 7, 8], size=pos_cols.size).astype(np.uint16) #computes cols with incorrect disparity\n",
        "\n",
        "              #Calibrations which will be used to discard changes to a pixel which are not allowed\n",
        "              calibrate_rows = (rows >= distance) & (rows < disp_image.shape[0] - distance)\n",
        "              calibrate_cols = (cols >= distance) & (cols < disp_image.shape[1] - distance)\n",
        "              calibrate_pos_cols = (pos_cols >= distance) & (pos_cols < disp_image.shape[1] - distance)\n",
        "              calibrate_neg_cols = (neg_cols >= distance) & (neg_cols < disp_image.shape[1] - distance)\n",
        "              calibrations = calibrate_rows & calibrate_cols & calibrate_pos_cols & calibrate_neg_cols\n",
        "\n",
        "              #Making narray of image indexes and corresponding rows, cols, pos_cols and neg_cols\n",
        "              rows = rows[calibrations]\n",
        "              cols = cols[calibrations]\n",
        "              pos_cols = pos_cols[calibrations]\n",
        "              neg_cols = neg_cols[calibrations]\n",
        "\n",
        "              result = np.empty(len(rows), dtype=np.dtype([('idx', 'uint8'), ('row', 'uint16'), ('col', 'uint16'), ('col_pos', 'uint16'), ('col_neg', 'uint16'), ]))\n",
        "              result['idx'] = np.full(rows.shape, idx, dtype=np.uint8)\n",
        "              result['row'] = rows\n",
        "              result['col'] = cols\n",
        "              result['col_pos'] = pos_cols\n",
        "              result['col_neg'] = neg_cols\n",
        "\n",
        "              return result\n",
        "\n",
        "      disparities = np.concatenate([filter(idx) for idx in range(TRAIN_START if train else VALID_START, TRAIN_END if train else VALID_END)])\n",
        "      np.save((TRAIN_DATA if train else VALID_DATA) + 'disparities.npy', disparities)"
      ],
      "metadata": {
        "id": "r8Yhx7tmSpgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atuucoSLPIiC"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class StereoCNN(nn.Module):\n",
        "\tdef __init__(self, in_channels=3, features=64, ksize=3, padding=1):\n",
        "          super().__init__()\n",
        "          self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=features, kernel_size=ksize, padding=padding)\n",
        "          self.conv2 = nn.Conv2d(in_channels=features, out_channels=features, kernel_size=ksize, padding=padding)\n",
        "          self.conv3 = nn.Conv2d(in_channels=features, out_channels=features, kernel_size=ksize, padding=padding)\n",
        "          self.conv4 = nn.Conv2d(in_channels=features, out_channels=features, kernel_size=ksize, padding=padding)\n",
        "\tdef forward(self, x):\n",
        "          x = F.relu(self.conv1(x))\n",
        "          x = F.relu(self.conv2(x))\n",
        "          x = F.relu(self.conv3(x))\n",
        "          x = self.conv4(x)\n",
        "          x = x.squeeze(3).squeeze(2) #dimensions of tensor at second and third dimension are 1, therefore they will be removed\n",
        "          return  F.normalize(x) #it normalizes vector with euclidean norm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PatchesExtractor(Dataset):\n",
        "\n",
        "  def __init__(self, tform, train=True):\n",
        "          self.transform = tform\n",
        "          self.disparity_data = load_disparity_data(train=train)\n",
        "          self.len = self.disparity_data.size\n",
        "          self.left_images = {}\n",
        "          self.right_images = {}\n",
        "          \n",
        "          for idx in range(TRAIN_START if train else VALID_START, TRAIN_END if train else VALID_END):\n",
        "                        self.left_images[idx] = get_image(idx, is_left=True)\n",
        "                        self.right_images[idx] = get_image(idx, is_left=False)\n",
        "\t#Method that extracts patches from particular image\n",
        "  def __getitem__(self, patch_idx):\n",
        "          patch_data = self.disparity_data[patch_idx]\n",
        "          image_idx = patch_data['idx']\n",
        "          row = patch_data['row']\n",
        "          col = patch_data['col']\n",
        "          col_pos = patch_data['col_pos']\n",
        "          col_neg = patch_data['col_neg']\n",
        "\n",
        "          patch_size = PATCH_SIZE\n",
        "          left_image = self.left_images[image_idx]\n",
        "          right_pos_image = self.right_images[image_idx]\n",
        "          rigth_neg_image = self.right_images[image_idx]\n",
        "\n",
        "          if self.transform:\n",
        "              left_patch = self.transform(left_image[(row - patch_size // 2):(row + patch_size // 2), (col - patch_size // 2):(col + patch_size // 2)])\n",
        "              right_positive_patch = self.transform(right_pos_image[(row - patch_size // 2):(row + patch_size // 2), (col_pos - patch_size // 2):(col_pos + patch_size // 2)])\n",
        "              right_negative_patch = self.transform(rigth_neg_image[(row - patch_size // 2):(row + patch_size // 2), (col_neg - patch_size // 2):(col_neg + patch_size // 2)])\n",
        "                                                    \n",
        "          return left_patch, right_positive_patch, right_negative_patch\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "metadata": {
        "id": "SZ614mlrSRyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_disparity_data(train=True)\n",
        "make_disparity_data(train=False)\n"
      ],
      "metadata": {
        "id": "xzbX6dvYbtwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "\n",
        "model = StereoCNN(in_channels=3)\n",
        "model.to(device)\n",
        "\n",
        "RGB_transform = transforms.Compose([\n",
        "\ttransforms.ToTensor(),\n",
        "\ttransforms.Normalize((0.4817, 0.5085, 0.5006), (0.3107, 0.3249, 0.3350))\n",
        "\t])\n",
        "GRAY_transform = transforms.Compose([\n",
        "\ttransforms.ToTensor(),\n",
        "    \ttransforms.Normalize((0.4999), (0.3180))\n",
        "\t])\n",
        "\n",
        "train_dataset = PatchesExtractor(tform=RGB_transform,train=True)\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "criterion = nn.TripletMarginLoss(margin=MARGIN)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LR)\n",
        "\n",
        "train_report = np.zeros((EPOCHS, ))\n",
        "num_batches = 0\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    train_loss = 0\n",
        "    epoch_batches = 0\n",
        "    model.train()\n",
        "    for left_patch, right_pos_patch, right_neg_patch in train_dataloader:\n",
        "      global num_batches\n",
        "      num_batches += 1\n",
        "      epoch_batches += 1\n",
        "      left_patch, right_pos_patch, right_neg_patch = left_patch.to(device), right_pos_patch.to(device), right_neg_patch.to(device)\n",
        "      left_output, right_pos_output, right_neg_output = model(left_patch), model(right_pos_patch), model(right_neg_patch)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss = criterion(left_output, right_pos_output, right_neg_output)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      if epoch_batches % 1000 == 0:\n",
        "        print(f\"Done {((BATCH_SIZE * epoch_batches) / len(train_dataset)) * 100:.3f} %\")\n",
        "        print('Train -> Loss: %.3f \\n' % (train_loss/ epoch_batches))\n",
        "\n",
        "    train_report[epoch] = train_loss / epoch_batches\n",
        "    torch.save(model, f\"train_model_{epoch}.pth\")\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)\n",
        "    if epoch == LR_CHANGE_AT_EPOCH:\n",
        "            for param in optimizer.param_groups:\n",
        "                  param['lr'] = LR_AFTER_10_EPOCHS\n",
        "\n",
        "np.save(TRAIN_DATA + 'training_report.npy', train_report)"
      ],
      "metadata": {
        "id": "A-3NgAmIYwCG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}